use anyhow::{bail, Result};
use async_compression::tokio::bufread::GzipDecoder;
#[cfg(feature = "extended")]
use charabia::Segment;
use either::Either;
use hashbrown::HashMap;
use lazy_static::lazy_static;
use pagefind_stem::{Algorithm, Stemmer};
use path_slash::PathExt as _;
use regex::Regex;
use std::collections::BTreeMap;
use std::io::Error;
use std::ops::Mul;
use std::path::{Path, PathBuf};
use tokio::fs::File;
use tokio::io::AsyncBufReadExt;
use tokio::io::{AsyncReadExt, BufReader};
use tokio::time::{sleep, Duration};

use crate::fragments::{PageAnchorData, PageFragment, PageFragmentData};
use crate::SearchOptions;
use parser::DomParser;

use self::parser::DomParserResult;
use self::splitting::get_discrete_words;

lazy_static! {
    static ref NEWLINES: Regex = Regex::new("(\n|\r\n)+").unwrap();
    static ref TRIM_NEWLINES: Regex = Regex::new("^[\n\r\\s]+|[\n\r\\s]+$").unwrap();
    static ref EXTRANEOUS_SPACES: Regex = Regex::new("\\s{2,}").unwrap();
    static ref PRIVATE_PAGEFIND: Regex = Regex::new("___PAGEFIND_[\\S]+\\s?").unwrap();
}

pub mod parser;
mod splitting;

#[derive(Debug, Clone, PartialEq)]
pub struct FossickedWord {
    pub position: u32,
    pub weight: u8,
}

#[derive(Debug, Clone)]
pub struct FossickedData {
    pub url: String,
    pub fragment: PageFragment,
    pub word_data: HashMap<String, Vec<FossickedWord>>,
    pub sort: BTreeMap<String, String>,
    pub has_custom_body: bool,
    pub force_inclusion: bool,
    pub has_html_element: bool,
    pub has_old_bundle_reference: bool,
    pub language: String,
}

#[derive(Debug)]
pub struct Fossicker {
    file_path: Option<PathBuf>,
    /// Built URLs should be relative to this directory
    root_path: Option<PathBuf>,
    page_url: Option<String>,
    synthetic_content: Option<String>,
    data: Option<DomParserResult>,
}

impl Fossicker {
    pub fn new_relative_to(file_path: PathBuf, root_path: PathBuf) -> Self {
        Self {
            file_path: Some(file_path),
            root_path: Some(root_path),
            page_url: None,
            synthetic_content: None,
            data: None,
        }
    }

    pub fn new_synthetic(
        file_path: Option<PathBuf>,
        page_url: Option<String>,
        contents: String,
    ) -> Self {
        Self {
            file_path,
            root_path: None,
            page_url,
            synthetic_content: Some(contents),
            data: None,
        }
    }

    pub fn new_with_data(url: String, data: DomParserResult) -> Self {
        Self {
            file_path: None,
            root_path: None,
            page_url: Some(url),
            synthetic_content: None,
            data: Some(data),
        }
    }

    async fn read_file(&mut self, options: &SearchOptions) -> Result<(), Error> {
        let Some(file_path) = &self.file_path else {
            return Ok(());
        }; // TODO: Change to thiserror
        let file = File::open(file_path).await?;

        let mut rewriter = DomParser::new(options);

        let mut br = BufReader::new(file);
        let mut buf = [0; 20000];

        let is_gzip = if let Ok(read) = br.fill_buf().await {
            read.len() >= 3 && read[0] == 0x1F && read[1] == 0x8B && read[2] == 0x08
        } else {
            false
        };

        if is_gzip {
            let mut br = GzipDecoder::new(br);
            while let Ok(read) = br.read(&mut buf).await {
                if read == 0 {
                    break;
                }
                if let Err(error) = rewriter.write(&buf[..read]) {
                    options.logger.error(format!(
                        "Failed to parse file {} â€” skipping this file. Error:\n{error}",
                        file_path.to_str().unwrap_or("[unknown file]"),
                    ));
                    return Ok(());
                }
            }
        } else {
            while let Ok(read) = br.read(&mut buf).await {
                if read == 0 {
                    break;
                }
                if let Err(error) = rewriter.write(&buf[..read]) {
                    options.logger.error(format!(
                        "Failed to parse file {} â€” skipping this file. Error:\n{error}",
                        file_path.to_str().unwrap_or("[unknown file]")
                    ));
                    return Ok(());
                }
            }
        }

        let mut data = rewriter.wrap();
        if let Some(forced_language) = &options.force_language {
            data.language = forced_language.clone();
        }

        self.data = Some(data);

        Ok(())
    }

    async fn read_synthetic(&mut self, options: &SearchOptions) -> Result<(), Error> {
        let Some(contents) = self.synthetic_content.as_ref() else {
            return Ok(());
        };

        let mut rewriter = DomParser::new(options);

        let mut br = BufReader::new(contents.as_bytes());
        let mut buf = [0; 20000];

        while let Ok(read) = br.read(&mut buf).await {
            if read == 0 {
                break;
            }
            if let Err(error) = rewriter.write(&buf[..read]) {
                options.logger.error(format!(
                    "Failed to parse file {} â€” skipping this file. Error:\n{error}",
                    &self
                        .file_path
                        .as_ref()
                        .map(|p| p.to_str())
                        .flatten()
                        .or(self.page_url.as_ref().map(|u| u.as_str()))
                        .unwrap_or("[unknown file]")
                ));
                return Ok(());
            }
        }

        let mut data = rewriter.wrap();
        if let Some(forced_language) = &options.force_language {
            data.language = forced_language.clone();
        }

        self.data = Some(data);

        Ok(())
    }

    fn parse_digest(
        &mut self,
        options: &SearchOptions,
    ) -> (
        String,
        HashMap<String, Vec<FossickedWord>>,
        Vec<(String, String, String, u32)>,
        usize,
    ) {
        let mut map: HashMap<String, Vec<FossickedWord>> = HashMap::new();
        let mut anchors = Vec::new();
        // TODO: push this error handling up a level and return an Err from parse_digest
        if self.data.as_ref().is_none() {
            return ("".into(), map, anchors, 0); // empty page result, will be dropped from search
        }
        let data = self.data.as_ref().unwrap();
        let stemmer = get_stemmer(&data.language);

        let mut content = String::with_capacity(data.digest.len());

        let mut store_word = |full_word: &str, word_index: usize, word_weight: u8| {
            let word = if let Some(stemmer) = &stemmer {
                stemmer.stem(&full_word).into_owned()
            } else {
                full_word.to_string()
            };

            let entry = FossickedWord {
                position: word_index.try_into().unwrap(),
                weight: word_weight,
            };
            if let Some(repeat) = map.get_mut(&word) {
                repeat.push(entry);
            } else {
                map.insert(word, vec![entry]);
            }
        };

        // TODO: Consider reading newlines and jump the word_index up some amount,
        // so that separate bodies of text don't return exact string
        // matches across the boundaries. Or otherwise use some marker byte for the boundary.

        // TODO: Configure this or use segmenting across all languages

        let segment_chunks = data.digest.split_whitespace();

        #[cfg(feature = "extended")]
        let should_segment = matches!(data.language.split('-').next().unwrap(), "zh" | "ja");

        #[cfg(feature = "extended")]
        let coarse_segments = segment_chunks.map(|seg| {
            if seg.starts_with("___") {
                Either::Left(seg)
            } else {
                if should_segment {
                    // Run a segmenter only for any languages which require it.
                    Either::Right(seg.segment_str())
                } else {
                    // Currently hesitant to run segmentation during indexing
                    // that we can't also run during search, since we don't
                    // ship a segmenter to the browser. This logic is easier
                    // to replicate in the JavaScript that parses a search query.
                    Either::Left(seg)
                }
            }
        });

        #[cfg(not(feature = "extended"))]
        let coarse_segments =
            segment_chunks.map(|s| Either::<&str, core::slice::Iter<&str>>::Left(s));

        let mut total_word_index = 0;
        let mut max_word_index = 0;
        let weight_multiplier = 24.0;
        let weight_max = 10.0;
        debug_assert!(((weight_max * weight_multiplier) as u8) < std::u8::MAX);

        let mut weight_stack: Vec<u8> = vec![(1.0 * weight_multiplier) as u8];

        let mut track_word = |word: &str, append_whitespace: bool| {
            if word.chars().next() == Some('_') {
                if word.starts_with("___PAGEFIND_ANCHOR___") {
                    if let Some((element_name, anchor_id)) =
                        word.replace("___PAGEFIND_ANCHOR___", "").split_once(':')
                    {
                        let element_text = data
                            .anchor_content
                            .get(anchor_id)
                            .map(|t| normalize_content(t))
                            .unwrap_or_default();

                        if let Some((_, element_id)) = anchor_id.split_once(':') {
                            anchors.push((
                                element_name.to_string(),
                                element_id.to_string(),
                                normalize_content(&element_text),
                                total_word_index as u32,
                            ));
                        }
                    }
                    return;
                }

                if word.starts_with("___PAGEFIND_WEIGHT___") {
                    let weight = word
                        .replace("___PAGEFIND_WEIGHT___", "")
                        .parse::<f32>()
                        .ok()
                        .unwrap_or(1.0);
                    if weight <= 0.0 {
                        weight_stack.push(0);
                    } else {
                        weight_stack.push(
                            (weight.clamp(0.0, weight_max).mul(weight_multiplier) as u8).max(1),
                        );
                    }
                    return;
                }

                // Auto weights are provided by the parser, and should only
                // apply if we aren't inside an explicitly weighted block,
                // in which case we should just inherit that weight.
                if word.starts_with("___PAGEFIND_AUTO_WEIGHT___") {
                    if weight_stack.len() == 1 {
                        let weight = word
                            .replace("___PAGEFIND_AUTO_WEIGHT___", "")
                            .parse::<f32>()
                            .ok()
                            .unwrap_or(1.0);
                        weight_stack
                            .push(weight.clamp(0.0, weight_max).mul(weight_multiplier) as u8);
                    } else {
                        weight_stack.push(weight_stack.last().cloned().unwrap_or_default());
                    }
                    return;
                }

                if word.starts_with("___END_PAGEFIND_WEIGHT___") {
                    weight_stack.pop();
                    return;
                }
            }

            // We use zero-width spaces as boundary values for some languages,
            // so we make sure that all are removed from the source content before going into the index.
            let normalized_word = word.replace('\u{200B}', "");
            if normalized_word.is_empty() {
                return;
            }

            content.push_str(&word.replace('\u{200B}', ""));
            if append_whitespace {
                content.push(' ');
            }
            #[cfg(feature = "extended")]
            if should_segment {
                content.push('\u{200B}');
            }
            let mut normalized_word = String::with_capacity(word.len());
            let mut possibly_compound = false;

            for mut c in word.chars() {
                let is_alpha = c.is_alphanumeric();
                if !is_alpha {
                    possibly_compound = true;
                }
                if is_alpha || options.include_characters.contains(&c) {
                    c.make_ascii_lowercase();
                    if c.is_uppercase() {
                        // Non-ascii uppercase can lower to multiple chars
                        normalized_word.extend(c.to_lowercase());
                    } else {
                        normalized_word.push(c);
                    }
                }
            }

            let word_weight = weight_stack.last().unwrap_or(&1);
            if !normalized_word.is_empty() {
                store_word(&normalized_word, total_word_index, *word_weight);
            }

            // For words that may be CompoundWords, also index them as their constituent parts
            if possibly_compound {
                let (word_parts, extras) = get_discrete_words(word);
                // Only proceed if the word was broken into multiple parts
                if word_parts.contains(|c: char| c.is_whitespace())
                    || (!normalized_word.starts_with(&word_parts))
                {
                    let part_words: Vec<_> = word_parts.split_whitespace().collect();

                    if !part_words.is_empty() {
                        // Index constituents of a compound word as a proportion of the
                        // weight of the full word.
                        let per_weight = (word_weight
                            / part_words.len().try_into().unwrap_or(std::u8::MAX))
                        .max(1);

                        // Only index two+ character words
                        for part_word in part_words.into_iter().filter(|w| w.len() > 1) {
                            store_word(part_word, total_word_index, per_weight);
                        }
                    }
                }
                // Additionally store any special extra characters we are given
                if let Some(extras) = extras {
                    for extra in extras {
                        store_word(&extra, total_word_index, *word_weight);
                    }
                }
            }

            max_word_index = total_word_index;
            total_word_index += 1;
        };

        for segment in coarse_segments {
            match segment {
                Either::Left(word) => {
                    track_word(word, true);
                }
                Either::Right(words) => {
                    let mut words = words.peekable();
                    while let Some(word) = words.next() {
                        track_word(word, words.peek().is_none());
                    }
                }
            };
        }
        if content.ends_with('\u{200B}') {
            content.pop();
        }
        if content.ends_with(' ') {
            content.pop();
        }
        (content, map, anchors, max_word_index + 1)
    }

    /// Removes private Pagefind sentinel values from content that would otherwise leak.
    /// This should probably be handled better by not inserting these flags here in the first place,
    /// though there's a chance we do want to process them when we arrive at indexing metadata.
    fn tidy_meta_and_filters(&mut self) {
        if let Some(data) = self.data.as_mut() {
            for filter in data.filters.values_mut() {
                for filter_val in filter.iter_mut() {
                    match PRIVATE_PAGEFIND.replace_all(filter_val, "") {
                        std::borrow::Cow::Borrowed(_) => { /* no-op, no replace happened */ }
                        std::borrow::Cow::Owned(s) => *filter_val = s,
                    }
                }
            }

            for meta in data.meta.values_mut() {
                match PRIVATE_PAGEFIND.replace_all(meta, "") {
                    std::borrow::Cow::Borrowed(_) => { /* no-op, no replace happened */ }
                    std::borrow::Cow::Owned(s) => *meta = s,
                }
            }
        }
    }

    async fn fossick_html(&mut self, options: &SearchOptions) {
        if self.synthetic_content.is_some() {
            while self.read_synthetic(options).await.is_err() {
                sleep(Duration::from_millis(1)).await;
            }
        } else {
            while self.read_file(options).await.is_err() {
                sleep(Duration::from_millis(1)).await;
            }
        }
    }

    pub async fn fossick(mut self, options: &SearchOptions) -> Result<FossickedData> {
        if (self.file_path.is_some() || self.synthetic_content.is_some()) && self.data.is_none() {
            self.fossick_html(options).await;
        };

        let (content, word_data, anchors, word_count) = self.parse_digest(options);
        self.tidy_meta_and_filters();

        let data = self.data.unwrap();
        let url = if let Some(url) = &self.page_url {
            url.clone()
        } else if let Some(path) = &self.file_path {
            if let Some(root) = &self.root_path {
                build_url(path, Some(root), options)
            } else {
                build_url(path, None, options)
            }
        } else {
            options
                .logger
                .error("Tried to index file with no specified URL or file path, ignoring.");
            bail!("Tried to index file with no specified URL or file path, ignoring.");
        };

        Ok(FossickedData {
            url: url.clone(),
            has_custom_body: data.has_custom_body,
            force_inclusion: data.force_inclusion,
            has_html_element: data.has_html_element,
            has_old_bundle_reference: data.has_old_bundle_reference,
            language: data.language,
            fragment: PageFragment {
                page_number: 0, // This page number is updated later once determined
                data: PageFragmentData {
                    url,
                    content,
                    filters: data.filters,
                    meta: data.meta,
                    word_count,
                    anchors: anchors
                        .into_iter()
                        .map(|(element, id, text, location)| PageAnchorData {
                            element,
                            id,
                            location,
                            text,
                        })
                        .collect(),
                },
            },
            word_data,
            sort: data.sort,
        })
    }
}

fn strip_index_html(url: &str) -> &str {
    if url.ends_with("/index.html") {
        &url[..url.len() - 10]
    } else if url == "index.html" {
        ""
    } else {
        url
    }
}

fn build_url(page_url: &Path, relative_to: Option<&Path>, options: &SearchOptions) -> String {
    let prefix = relative_to.unwrap_or(&options.site_source);

    let url = if let Ok(trimmed) = page_url.strip_prefix(prefix) {
        trimmed
    } else if page_url.is_relative() {
        page_url
    } else {
        options.logger.error(format!(
            "Absolute file was found that does not start with the source directory. Source: {:?}\nFile: {:?}",
            prefix,
            page_url
        ));
        return "/unknown/".to_string();
    };

    let final_url: String = if !options.keep_index_url {
        strip_index_html(&url.to_slash_lossy()).to_string()
    } else {
        url.to_slash_lossy().to_owned().to_string()
    };

    format!("/{}", final_url)
}

fn normalize_content(content: &str) -> String {
    let content = html_escape::decode_html_entities(content);
    let content = TRIM_NEWLINES.replace_all(&content, "");
    let content = NEWLINES.replace_all(&content, " ");
    let content = EXTRANEOUS_SPACES.replace_all(&content, " ");

    content.to_string()
}

// TODO: These language codes are duplicated with pagefind_web's Cargo.toml
fn get_stemmer(lang: &str) -> Option<Stemmer> {
    match lang.split('-').next().unwrap() {
        "ar" => Some(Stemmer::create(Algorithm::Arabic)),
        "hy" => Some(Stemmer::create(Algorithm::Armenian)),
        "eu" => Some(Stemmer::create(Algorithm::Basque)),
        "ca" => Some(Stemmer::create(Algorithm::Catalan)),
        "da" => Some(Stemmer::create(Algorithm::Danish)),
        "nl" => Some(Stemmer::create(Algorithm::Dutch)),
        "en" => Some(Stemmer::create(Algorithm::English)),
        "fi" => Some(Stemmer::create(Algorithm::Finnish)),
        "fr" => Some(Stemmer::create(Algorithm::French)),
        "de" => Some(Stemmer::create(Algorithm::German)),
        "el" => Some(Stemmer::create(Algorithm::Greek)),
        "hi" => Some(Stemmer::create(Algorithm::Hindi)),
        "hu" => Some(Stemmer::create(Algorithm::Hungarian)),
        "id" => Some(Stemmer::create(Algorithm::Indonesian)),
        "ga" => Some(Stemmer::create(Algorithm::Irish)),
        "it" => Some(Stemmer::create(Algorithm::Italian)),
        "lt" => Some(Stemmer::create(Algorithm::Lithuanian)),
        "ne" => Some(Stemmer::create(Algorithm::Nepali)),
        "no" => Some(Stemmer::create(Algorithm::Norwegian)),
        "pt" => Some(Stemmer::create(Algorithm::Portuguese)),
        "ro" => Some(Stemmer::create(Algorithm::Romanian)),
        "ru" => Some(Stemmer::create(Algorithm::Russian)),
        "sr" => Some(Stemmer::create(Algorithm::Serbian)),
        "es" => Some(Stemmer::create(Algorithm::Spanish)),
        "sv" => Some(Stemmer::create(Algorithm::Swedish)),
        "ta" => Some(Stemmer::create(Algorithm::Tamil)),
        "tr" => Some(Stemmer::create(Algorithm::Turkish)),
        "yi" => Some(Stemmer::create(Algorithm::Yiddish)),
        _ => None,
    }
}

#[cfg(test)]
mod tests {
    use crate::PagefindInboundConfig;
    use twelf::Layer;

    use super::*;

    #[test]
    fn normalizing_content() {
        let input = "\nHello  Wor\n ld? \n \n";
        let output = normalize_content(input);

        assert_eq!(&output, "Hello Wor ld?");
    }

    fn test_opts() -> SearchOptions {
        std::env::set_var("PAGEFIND_SOURCE", "somewhere");
        let config =
            PagefindInboundConfig::with_layers(&[Layer::Env(Some("PAGEFIND_".into()))]).unwrap();
        SearchOptions::load(config).unwrap()
    }

    async fn test_fossick(s: String) -> Fossicker {
        let mut f = Fossicker {
            file_path: Some("test/index.html".into()),
            root_path: None,
            page_url: Some("/test/".into()),
            synthetic_content: Some(s),
            data: None,
        };

        _ = f.read_synthetic(&test_opts()).await;

        f
    }

    #[tokio::test]
    async fn parse_file() {
        let mut f =
            test_fossick(["<html><body>", "<p>Hello World!</p>", "</body></html>"].concat()).await;

        let (digest, words, _, _) = f.parse_digest(&test_opts());

        assert_eq!(digest, "Hello World!".to_string());
        assert_eq!(
            words,
            HashMap::from_iter([
                (
                    "hello".to_string(),
                    vec![FossickedWord {
                        position: 0,
                        weight: 1 * 24
                    }]
                ),
                (
                    "world".to_string(),
                    vec![FossickedWord {
                        position: 1,
                        weight: 1 * 24
                    }]
                )
            ])
        );
    }

    #[tokio::test]
    async fn parse_chars() {
        let mut f = test_fossick(
            [
                "<html><body>",
                "<p>He&amp;llo htmltag&lt;head&gt; *before mid*dle after*</p>",
                "</body></html>",
            ]
            .concat(),
        )
        .await;

        let mut opts = test_opts();
        opts.include_characters.extend(['<', '>', '*']);
        let (digest, words, _, _) = f.parse_digest(&opts);

        assert_eq!(
            digest,
            "He&llo htmltag<head> *before mid*dle after*.".to_string()
        );
        assert_eq!(
            words,
            HashMap::from_iter([
                (
                    "he".to_string(),
                    vec![FossickedWord {
                        position: 0,
                        weight: 12
                    }]
                ),
                (
                    "llo".to_string(),
                    vec![FossickedWord {
                        position: 0,
                        weight: 12
                    }]
                ),
                (
                    "hello".to_string(),
                    vec![FossickedWord {
                        position: 0,
                        weight: 24
                    }]
                ),
                (
                    "htmltag<head>".to_string(),
                    vec![FossickedWord {
                        position: 1,
                        weight: 24
                    }]
                ),
                (
                    "htmltag".to_string(),
                    vec![FossickedWord {
                        position: 1,
                        weight: 12
                    }]
                ),
                (
                    "head".to_string(),
                    vec![FossickedWord {
                        position: 1,
                        weight: 12
                    }]
                ),
                (
                    "*before".to_string(),
                    vec![FossickedWord {
                        position: 2,
                        weight: 24
                    }]
                ),
                (
                    "before".to_string(),
                    vec![FossickedWord {
                        position: 2,
                        weight: 24
                    }]
                ),
                (
                    "mid*dle".to_string(),
                    vec![FossickedWord {
                        position: 3,
                        weight: 24
                    }]
                ),
                (
                    "mid".to_string(),
                    vec![FossickedWord {
                        position: 3,
                        weight: 12
                    }]
                ),
                (
                    "dle".to_string(),
                    vec![FossickedWord {
                        position: 3,
                        weight: 12
                    }]
                ),
                (
                    "after*".to_string(),
                    vec![FossickedWord {
                        position: 4,
                        weight: 24
                    }]
                )
            ])
        );
    }

    #[tokio::test]
    async fn parse_weighted_file() {
        let mut f = test_fossick(
            [
                "<html><body>",
                "<div>The",
                "<p data-pagefind-weight='2'>Quick Brown</p>",
                "Fox",
                "<p data-pagefind-weight='0.5'>Jumps Over</p>",
                "<p data-pagefind-weight='0.00001'>Ryan</p></div>",
                "</body></html>",
            ]
            .concat(),
        )
        .await;

        let (digest, words, _, _) = f.parse_digest(&test_opts());

        assert_eq!(digest, "The Quick Brown. Fox Jumps Over. Ryan.".to_string());
        assert_eq!(
            words,
            HashMap::from_iter([
                (
                    "the".to_string(),
                    vec![FossickedWord {
                        position: 0,
                        weight: 1 * 24
                    }]
                ),
                (
                    "quick".to_string(),
                    vec![FossickedWord {
                        position: 1,
                        weight: 2 * 24
                    }]
                ),
                (
                    "brown".to_string(),
                    vec![FossickedWord {
                        position: 2,
                        weight: 2 * 24
                    }]
                ),
                (
                    "fox".to_string(),
                    vec![FossickedWord {
                        position: 3,
                        weight: 1 * 24
                    }]
                ),
                (
                    "jumps".to_string(),
                    vec![FossickedWord {
                        position: 4,
                        weight: 12
                    }]
                ),
                (
                    "over".to_string(),
                    vec![FossickedWord {
                        position: 5,
                        weight: 12
                    }]
                ),
                (
                    "ryan".to_string(),
                    vec![FossickedWord {
                        position: 6,
                        weight: 1
                    }]
                )
            ])
        );
    }

    #[tokio::test]
    async fn parse_auto_weighted_file() {
        let mut f = test_fossick(
            [
                "<html><body>",
                "<h1>Pagefind</h1>",
                "<h2>Pagefind</h2>",
                "<h3>Pagefind</h3>",
                "<h4>Pagefind</h4>",
                "<h5>Pagefind</h5>",
                "<h6>Pagefind</h6>",
                "<p>Pagefind</p>",
                "<div data-pagefind-weight='0'><h1>Pagefind</h1></div>",
                "</body></html>",
            ]
            .concat(),
        )
        .await;

        let (_, words, _, _) = f.parse_digest(&test_opts());

        assert_eq!(
            words,
            HashMap::from_iter([(
                "pagefind".to_string(),
                vec![
                    FossickedWord {
                        position: 0,
                        weight: 7 * 24
                    },
                    FossickedWord {
                        position: 1,
                        weight: 6 * 24
                    },
                    FossickedWord {
                        position: 2,
                        weight: 5 * 24
                    },
                    FossickedWord {
                        position: 3,
                        weight: 4 * 24
                    },
                    FossickedWord {
                        position: 4,
                        weight: 3 * 24
                    },
                    FossickedWord {
                        position: 5,
                        weight: 2 * 24
                    },
                    FossickedWord {
                        position: 6,
                        weight: 1 * 24
                    },
                    FossickedWord {
                        position: 7,
                        weight: 0 * 24
                    }
                ]
            )])
        );
    }

    #[tokio::test]
    async fn parse_bad_weights() {
        let mut f = test_fossick(
            [
                "<html><body>",
                "<p data-pagefind-weight='lots'>The</p>",
                "<p data-pagefind-weight='99999999'>Quick</p>",
                "<p data-pagefind-weight='-1234'>Brown</p>",
                "<p data-pagefind-weight='65.4'>Fox</p>",
                "</body></html>",
            ]
            .concat(),
        )
        .await;

        let (_, words, _, _) = f.parse_digest(&test_opts());

        assert_eq!(
            words,
            HashMap::from_iter([
                (
                    "the".to_string(),
                    vec![FossickedWord {
                        position: 0,
                        weight: 24
                    }]
                ),
                (
                    "quick".to_string(),
                    vec![FossickedWord {
                        position: 1,
                        weight: 240
                    }]
                ),
                (
                    "brown".to_string(),
                    vec![FossickedWord {
                        position: 2,
                        weight: 0
                    }]
                ),
                (
                    "fox".to_string(),
                    vec![FossickedWord {
                        position: 3,
                        weight: 240
                    }]
                )
            ])
        );
    }

    #[tokio::test]
    async fn parse_nbsp() {
        let mut f = test_fossick(
            [
                "<html lang='ja'><body>",
                "<p>Hello&nbsp;ğŸ‘‹</p>",
                "</body></html>",
            ]
            .concat(),
        )
        .await;

        let (_, words, _, _) = f.parse_digest(&test_opts());

        let mut words = words.keys().collect::<Vec<_>>();
        words.sort();
        assert_eq!(words, vec!["hello", "ğŸ‘‹"]);
    }

    #[cfg(feature = "extended")]
    #[tokio::test]
    async fn parse_weights_through_segmentation() {
        let mut f = test_fossick(
            [
                "<html lang='zh'><body>",
                "<h1 id='my-title'>å“å‘€ï¼ æˆ‘çš„é”™ã€‚</h1>",
                "</body></html>",
            ]
            .concat(),
        )
        .await;

        let (content, words, _, _) = f.parse_digest(&test_opts());

        let mut words = words.keys().collect::<Vec<_>>();
        words.sort();
        assert_eq!(words, vec!["å“å‘€", "æˆ‘", "çš„", "é”™"]);

        assert_eq!(
            content,
            "å“å‘€\u{200b}ï¼ \u{200b}æˆ‘\u{200b}çš„\u{200b}é”™\u{200b}ã€‚"
        );
    }

    #[cfg(feature = "extended")]
    #[tokio::test]
    async fn segmentation_parity_when_presplitting() {
        fn get_comparison_segmentations(full_input: &'static str) -> (Vec<String>, Vec<String>) {
            let chunked_input = full_input
                .split_whitespace()
                .filter(|w| !w.starts_with("___"))
                .collect::<Vec<_>>();
            let clean_input = chunked_input.join(" ");

            let mut legitimate_output = clean_input
                .as_str()
                .segment_str()
                .filter(|w| w.chars().any(|c| !c.is_whitespace()))
                .map(Into::into)
                .collect::<Vec<_>>();
            let mut chunked_output = chunked_input
                .into_iter()
                .flat_map(|inp| {
                    inp.segment_str()
                        .filter(|w| w.chars().any(|c| !c.is_whitespace()))
                        .collect::<Vec<_>>()
                })
                .map(Into::into)
                .collect::<Vec<_>>();

            legitimate_output.sort();
            chunked_output.sort();
            (legitimate_output, chunked_output)
        }
        {
            let full_zh_input = "___PAGEFIND_AUTO_WEIGHT___7 æ“æœ‰é ç«¯å¸³è™Ÿæ¬Šé™ ___END_PAGEFIND_WEIGHT___

        æˆ‘å€‘å»ºè­°å¤§å¤šæ•¸å…·æœ‰é ç«¯å¸³è™Ÿæ¬Šé™çš„ä½¿ç”¨è€…ï¼Œæ¡ç”¨ ___PAGEFIND_ANCHOR___a:0:my-link Certbot é€™å€‹ ACME å®¢æˆ¶ç«¯ã€‚å®ƒå¯ä»¥è‡ªå‹•åŸ·è¡Œæ†‘è­‰çš„é ’ç™¼ã€å®‰è£ï¼Œç”šè‡³ä¸éœ€è¦åœæ­¢ä½ çš„ä¼ºæœå™¨ï¼›Certbot ä¹Ÿæä¾›å°ˆå®¶æ¨¡å¼ï¼Œçµ¦ä¸æƒ³è¦è‡ªå‹•è¨­å®šçš„ä½¿ç”¨è€…ã€‚Certbot æ“ä½œç°¡å–®ï¼Œé©ç”¨æ–¼è¨±å¤šç³»çµ±ï¼›ä¸¦ä¸”å…·æœ‰å®Œå–„çš„æ–‡æª”ã€‚åƒè€ƒ Certbot å®˜ç¶²ï¼Œä»¥ç²å–å°æ–¼ä¸åŒç³»çµ±å’Œç¶²é ä¼ºæœå™¨çš„æ“ä½œèªªæ˜ã€‚

        å¦‚æœ Certbot ä¸èƒ½æ»¿è¶³ä½ çš„éœ€æ±‚ï¼Œæˆ–æ˜¯ä½ æƒ³å˜—è©¦åˆ¥çš„å®¢æˆ¶ç«¯ï¼Œé‚„æœ‰å¾ˆå¤š ACME ç”¨æˆ¶ç«¯å¯ä¾›é¸æ“‡ã€‚åœ¨ä½ é¸å®š ACME å®¢æˆ¶ç«¯è»Ÿé«”å¾Œï¼Œè«‹åƒé–±è©²å®¢æˆ¶ç«¯çš„æ–‡æª”ã€‚
        ___PAGEFIND_WEIGHT___44
        å¦‚æœä½ æ­£åœ¨å˜—è©¦ä½¿ç”¨ä¸åŒçš„ ACME ç”¨æˆ¶ç«¯ï¼Œè«‹ä½¿ç”¨æˆ‘å€‘çš„æ¸¬è©¦ç’°å¢ƒä»¥å…è¶…éæ†‘è­‰é ’ç™¼èˆ‡æ›´æ–°çš„é€Ÿç‡é™åˆ¶ã€‚
        æ²’æœ‰é ç«¯å¸³è™Ÿæ¬Šé™

        åœ¨æ²’æœ‰é ç«¯å¸³è™Ÿæ¬Šé™çš„æƒ…æ³ä¸‹ï¼Œæœ€å¥½çš„è¾¦æ³•æ˜¯ä½¿ç”¨æœå‹™æ¥­è€…æ‰€æä¾›çš„ç¾æœ‰æ”¯æ´ã€‚å¦‚æœä½ çš„æ¥­è€…æ”¯æ´ ___PAGEFIND_ANCHOR___a:1:my-second-link Letâ€™s Encryptï¼Œé‚£éº¼ä»–å€‘å°±èƒ½å¹«åŠ©ä½ ç”³è«‹å…è²»æ†‘è­‰ï¼›å®‰è£ä¸¦è¨­å®šè‡ªå‹•æ›´æ–°ã€‚æŸäº›æ¥­è€…æœƒéœ€è¦ä½ åœ¨æ§åˆ¶ä»‹é¢æˆ–è¯ç¹«å®¢æœä»¥é–‹å•Ÿ Letâ€™s Encrypt æœå‹™ã€‚ä¹Ÿæœ‰äº›æ¥­è€…æœƒç‚ºæ‰€æœ‰å®¢æˆ¶è‡ªå‹•è¨­å®šä¸¦å®‰è£æ†‘è­‰ã€‚

        æŸ¥çœ‹æ”¯æ´ Letâ€™s Encrypt çš„æ¥­è€…åˆ—è¡¨ï¼Œç¢ºèªä½ æä¾›å•†çš„æ˜¯å¦æœ‰å‡ºç¾åœ¨åˆ—è¡¨ä¸Šã€‚å¦‚æœæœ‰çš„è©±ï¼Œè«‹æŒ‰ç…§ä»–å€‘çš„æ–‡æª”è¨­å®š Letâ€™s Encrypt æ†‘è­‰ã€‚ ___END_PAGEFIND_WEIGHT___";

            let (legitimate_zh_output, chunked_zh_output) =
                get_comparison_segmentations(full_zh_input);
            assert_eq!(legitimate_zh_output, chunked_zh_output);
        }

        {
            let full_zh_cn_input = "æ²¡æœ‰å‘½ä»¤è¡Œè®¿é—®æƒé™

        åœ¨æ²¡æœ‰å‘½ä»¤è¡Œè®¿é—®æƒé™çš„æƒ…å†µä¸‹ï¼Œ___PAGEFIND_AUTO_WEIGHT___7 æœ€å¥½çš„åŠæ³•æ˜¯ä½¿ç”¨æ‚¨æ‰˜ç®¡æœåŠ¡æä¾›å•†æä¾›çš„å†…ç½®åŠŸèƒ½ã€‚ æ”¯æŒ Letâ€™s Encrypt çš„æœåŠ¡å•†èƒ½æ›¿æ‚¨è‡ªåŠ¨å®Œæˆå…è´¹è¯ä¹¦çš„ç”³è¯·ã€å®‰è£…ã€ç»­æœŸæ­¥éª¤ã€‚ æŸäº›æœåŠ¡å•†å¯èƒ½éœ€è¦æ‚¨åœ¨æ§åˆ¶é¢æ¿ä¸­å¼€å¯ç›¸å…³é€‰é¡¹ï¼Œ ä¹Ÿæœ‰ä¸€äº›æœåŠ¡å•†ä¼šè‡ªåŠ¨ä¸ºæ‰€æœ‰å®¢æˆ·ç”³è¯·å¹¶å®‰è£…è¯ä¹¦ã€‚

        å¦‚æœæ‚¨çš„æœåŠ¡å•†å­˜åœ¨äºæˆ‘ä»¬çš„æœåŠ¡å•†åˆ—è¡¨ä¸­ï¼Œ å‚ç…§å…¶æ–‡æ¡£è®¾ç½® Letâ€™s Encrypt ___END_PAGEFIND_WEIGHT___ è¯ä¹¦å³å¯ã€‚

        å¦‚æœæ‚¨çš„æ‰˜ç®¡æœåŠ¡æä¾›å•†ä¸æ”¯æŒ ___PAGEFIND_ANCHOR___a:0:my-link Letâ€™s Encryptï¼Œæ‚¨å¯ä»¥ä¸ä»–ä»¬è”ç³»è¯·æ±‚æ”¯æŒã€‚ æˆ‘ä»¬å°½åŠ›ä½¿æ·»åŠ  Letâ€™s Encrypt æ”¯æŒå˜å¾—éå¸¸å®¹æ˜“ï¼Œæä¾›å•†ï¼ˆæ³¨ï¼šéä¸­å›½å›½å†…æä¾›å•†ï¼‰é€šå¸¸å¾ˆä¹æ„å¬å–å®¢æˆ·çš„å»ºè®®ï¼

        å¦‚æœæ‚¨çš„æ‰˜ç®¡æœåŠ¡æä¾›å•†ä¸æƒ³é›†æˆ Letâ€™s Encryptï¼Œä½†æ”¯æŒä¸Šä¼ è‡ªå®šä¹‰è¯ä¹¦ï¼Œæ‚¨å¯ä»¥åœ¨è‡ªå·±çš„è®¡ç®—æœºä¸Šå®‰è£… Certbot å¹¶ä½¿ç”¨æ‰‹åŠ¨æ¨¡å¼ï¼ˆManual Modeï¼‰ã€‚ åœ¨æ‰‹åŠ¨æ¨¡å¼ä¸‹ï¼Œæ‚¨éœ€è¦å°†æŒ‡å®šæ–‡ä»¶ä¸Šä¼ åˆ°æ‚¨çš„ç½‘ç«™ä»¥è¯æ˜æ‚¨çš„æ§åˆ¶æƒã€‚ ç„¶åï¼ŒCertbot å°†è·å–æ‚¨å¯ä»¥ä¸Šä¼ åˆ°æä¾›å•†çš„è¯ä¹¦ã€‚ æˆ‘ä»¬ä¸å»ºè®®ä½¿ç”¨æ­¤é€‰é¡¹ï¼Œå› ä¸ºå®ƒéå¸¸è€—æ—¶ï¼Œå¹¶ä¸”æ‚¨éœ€è¦åœ¨è¯ä¹¦è¿‡æœŸæ—¶é‡å¤æ­¤æ­¥éª¤ã€‚ å¯¹äºå¤§å¤šæ•°äººæ¥è¯´ï¼Œæœ€å¥½ä»æä¾›å•†å¤„è¯·æ±‚ Letâ€™s Encrypt æ”¯æŒã€‚è‹¥æ‚¨çš„æä¾›å•†ä¸æ‰“ç®—å…¼å®¹ï¼Œå»ºè®®æ‚¨æ›´æ¢æä¾›å•†ã€‚
        è·å–å¸®åŠ©

        å¦‚æœæ‚¨å¯¹é€‰æ‹© ACME å®¢æˆ·ç«¯ï¼Œä½¿ç”¨ç‰¹å®šå®¢æˆ·ç«¯æˆ–ä¸ Letâ€™s Encrypt ç›¸å…³çš„ä»»ä½•å…¶ä»–å†…å®¹æœ‰ç–‘é—®ï¼Œè¯·å‰å¾€æˆ‘ä»¬çš„ç¤¾åŒºè®ºå›è·å–å¸®åŠ©ã€‚";

            let (legitimate_zh_cn_output, chunked_zh_cn_output) =
                get_comparison_segmentations(full_zh_cn_input);
            assert_eq!(legitimate_zh_cn_output, chunked_zh_cn_output);
        }

        {
            let full_ja_input = "___PAGEFIND_AUTO_WEIGHT___7 ã‚·ã‚§ãƒ«ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©ã‚’æŒã£ã¦ã„ã‚‹å ´åˆ

            ã‚·ã‚§ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ãŒã§ãã‚‹ã»ã¨ã‚“ã©ã®äººã«ã¯ã€Certbot ã¨ã„ã† ACME ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ã†ã®ãŒãŠã™ã™ã‚ã§ã™ã€‚ ___END_PAGEFIND_WEIGHT___ è¨¼æ˜æ›¸ã®ç™ºè¡Œã¨ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’ã€ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ ã‚¼ãƒ­ã§è‡ªå‹•åŒ–ã§ãã¾ã™ã€‚ è‡ªå‹•è¨­å®šã‚’ä½¿ã„ãŸããªã„äººã®ãŸã‚ã«ã€ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãƒ¢ãƒ¼ãƒ‰ã‚‚ç”¨æ„ã•ã‚Œã¦ã„ã¾ã™ã€‚ ã¨ã¦ã‚‚ç°¡å˜ã«ä½¿ãˆã€å¤šæ•°ã®ã‚ªãƒšãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã§å‹•ä½œã—ã€ãŸãã•ã‚“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚‚ã‚ã‚Šã¾ã™ã€‚ Certbot ã®ã‚¦ã‚§ãƒ–ã‚µã‚¤ãƒˆã§ã¯ã€å„ã‚ªãƒšãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã‚„ã‚¦ã‚§ãƒ–ã‚µãƒ¼ãƒãƒ¼ã”ã¨ã®å€‹åˆ¥ã®è¨­å®šæ–¹æ³•ã«ã¤ã„ã¦è§£èª¬ã•ã‚Œã¦ã„ã¾ã™ã€‚

            Certbot ãŒã‚ãªãŸã®è¦ä»¶ã‚’æº€ãŸã•ãªã„å ´åˆã‚„ã€ä»–ã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’è©¦ã—ã¦ã¿ãŸã„å ´åˆã«ã¯ã€Certbot ã®ä»–ã«ã‚‚ãŸãã•ã‚“ã® ACME ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãŒåˆ©ç”¨ã§ãã¾ã™ã€‚ ACME ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’è‡ªåˆ†ã§é¸ã‚“ã å ´åˆã¯ã€ãã®ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

            åˆ¥ã® ACME ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ã£ã¦å®Ÿé¨“ã‚’è¡Œã†å ´åˆã¯ã€ ___PAGEFIND_ANCHOR___a:0:my-link ç§ãŸã¡ãŒç”¨æ„ã—ãŸã‚¹ãƒ†ãƒ¼ã‚¸ãƒ³ã‚°ç’°å¢ƒã‚’åˆ©ç”¨ã—ã¦ã€ãƒ¬ãƒ¼ãƒˆãƒ»ãƒªãƒŸãƒƒãƒˆã®åˆ¶é™ã‚’å—ã‘ãªã„ã‚ˆã†ã«æ°—ã‚’ã¤ã‘ã¦ãã ã•ã„ã€‚
            ã‚·ã‚§ãƒ«ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©ã‚’æŒã£ã¦ã„ãªã„å ´åˆ

            ã‚·ã‚§ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ãŒã§ããªã„å ´åˆã« Letâ€™s Encrypt ã‚’åˆ©ç”¨ã™ã‚‹ä¸€ç•ªè‰¯ã„æ–¹æ³•ã¯ã€ãƒ›ã‚¹ãƒ†ã‚£ãƒ³ã‚°ãƒ»ãƒ—ãƒ­ãƒã‚¤ãƒ€ãŒç”¨æ„ã—ãŸã‚µãƒãƒ¼ãƒˆã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã§ã™ã€‚ ã‚‚ã—ã€ã‚ãªãŸãŒåˆ©ç”¨ã™ã‚‹ãƒ›ã‚¹ãƒ†ã‚£ãƒ³ã‚°ãƒ»ãƒ—ãƒ­ãƒã‚¤ãƒ€ãŒ Letâ€™s Encrypt ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹å ´åˆã€ã‚ãªãŸã®ä»£ã‚ã‚Šã«ç„¡æ–™ã®è¨¼æ˜æ›¸ã‚’ãƒªã‚¯ã‚¨ã‚¹ãƒˆã€ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã€è‡ªå‹•çš„ã«æœ€æ–°ã®çŠ¶æ…‹ã«æ›´æ–°ã—ã¦ãã‚Œã¾ã™ã€‚ ä¸€éƒ¨ã®ãƒ›ã‚¹ãƒ†ã‚£ãƒ³ã‚°ãƒ»ãƒ—ãƒ­ãƒã‚¤ãƒ€ã§ã¯ã€ã“ã®æ©Ÿèƒ½ã¯è‡ªåˆ†ã§è¨­å®šã‹ã‚‰æœ‰åŠ¹ã«ã™ã‚‹å¿…è¦ãŒã‚ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚ ãã‚Œä»¥å¤–ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ã§ã¯ã€ã™ã¹ã¦ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãŸã‚ã«ã€è‡ªå‹•ã§è¨¼æ˜æ›¸ãŒç™ºè¡Œãƒ»ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã‚‹ã‚ˆã†ã«ãªã£ã¦ã„ã¾ã™ã€‚

            ã‚ãªãŸãŒåˆ©ç”¨ã—ã¦ã„ã‚‹ãƒ›ã‚¹ãƒ†ã‚£ãƒ³ã‚°ãƒ»ãƒ—ãƒ­ãƒã‚¤ãƒ€ãŒ Letâ€™s Encrypt ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹ã‹ã©ã†ã‹ã¯ã€ ãƒ›ã‚¹ãƒ†ã‚£ãƒ³ã‚°ãƒ»ãƒ—ãƒ­ãƒã‚¤ãƒ€ã®ãƒªã‚¹ãƒˆã§ç¢ºèªã—ã¦ãã ã•ã„ã€‚ ã‚‚ã—ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ãƒ›ã‚¹ãƒ†ã‚£ãƒ³ã‚°ãƒ»ãƒ—ãƒ­ãƒã‚¤ãƒ€ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ã«æ›¸ã‹ã‚Œã¦ã„ã‚‹ Letâ€™s Encrypt ã®è¨­å®šæ–¹æ³•ã«å¾“ã£ã¦ãã ã•ã„ã€‚";

            let (legitimate_ja_output, chunked_ja_output) =
                get_comparison_segmentations(full_ja_input);
            assert_eq!(legitimate_ja_output, chunked_ja_output);
        }
    }

    #[cfg(not(target_os = "windows"))]
    #[test]
    fn building_url() {
        std::env::set_var("PAGEFIND_SITE", "hello/world");
        let config =
            PagefindInboundConfig::with_layers(&[Layer::Env(Some("PAGEFIND_".into()))]).unwrap();
        let opts = SearchOptions::load(config).unwrap();

        let cwd = std::env::current_dir().unwrap();

        let p: PathBuf = cwd.join::<PathBuf>("hello/world/index.html".into());
        assert_eq!(&build_url(&p, None, &opts), "/");

        let p: PathBuf = cwd.join::<PathBuf>("hello/world/about/index.html".into());
        assert_eq!(&build_url(&p, None, &opts), "/about/");

        let p: PathBuf = cwd.join::<PathBuf>("hello/world/about.html".into());
        assert_eq!(&build_url(&p, None, &opts), "/about.html");

        let p: PathBuf = cwd.join::<PathBuf>("hello/world/about/index.htm".into());
        assert_eq!(&build_url(&p, None, &opts), "/about/index.htm");

        let p: PathBuf = cwd.join::<PathBuf>("hello/world/index.html".into());
        let root: PathBuf = cwd.join::<PathBuf>("hello".into());
        assert_eq!(&build_url(&p, Some(&root), &opts), "/world/");
    }

    #[cfg(target_os = "windows")]
    #[test]
    fn building_windows_urls() {
        std::env::set_var("PAGEFIND_SITE", "C:\\hello\\world");
        let config =
            PagefindInboundConfig::with_layers(&[Layer::Env(Some("PAGEFIND_".into()))]).unwrap();
        let opts = SearchOptions::load(config).unwrap();

        let p: PathBuf = "C:\\hello\\world\\index.html".into();
        assert_eq!(&build_url(&p, None, &opts), "/");

        let p: PathBuf = "C:\\hello\\world\\about\\index.html".into();
        assert_eq!(&build_url(&p, None, &opts), "/about/");

        let p: PathBuf = "C:\\hello\\world\\about\\index.htm".into();
        assert_eq!(&build_url(&p, None, &opts), "/about/index.htm");
    }
}
